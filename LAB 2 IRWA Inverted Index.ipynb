{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxl6OqtZBhrhc4oiMEifJP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1). Lets assume you have the following corpus.  Doc 1: breakthrough drug for schizophrenia  Doc 2: new schizophrenia drug  Doc 3: new approach for treatment of schizophrenia  Doc 4: new hopes for schizophrenia patients"],"metadata":{"id":"kRQnM0lA_lVp"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZlSNoUDm1sR7","executionInfo":{"status":"ok","timestamp":1664113380826,"user_tz":-330,"elapsed":33567,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"9eb70e75-9cff-4925-87a2-72254761c690"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/IRWA LAB 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGYocKS4408s","executionInfo":{"status":"ok","timestamp":1664113486698,"user_tz":-330,"elapsed":462,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"439d9fec-a94a-414e-d9bf-b9aecd1d0ab8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/IRWA LAB 2\n"]}]},{"cell_type":"markdown","source":["a). Write the code to build the inverted index for the above corpus."],"metadata":{"id":"Qp0jXf8H_hL1"}},{"cell_type":"code","source":["import os\n","\n","def InvertedIndex():\n","  dictionary=dict()\n","  directory = os.getcwd()+'/inverted'\n","\n","  #print(directory)\n","\n","  files=os.listdir(directory)\n","  #print(files)\n","\n","  for file in files:\n","    with open(os.getcwd()+'/inverted/'+file,'r')as f:\n","      #print(f)\n","      words=f.read().lower().split()\n","      print(words)\n","      for word in words:\n","        if word not in dictionary:\n","          dictionary[word]=[file]\n","        else:\n","          dictionary[word].append(file)\n","  return(dictionary)"],"metadata":{"id":"UQV9WVZf5OwV","executionInfo":{"status":"ok","timestamp":1664116526811,"user_tz":-330,"elapsed":475,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["b). Write suitable code to do the following retrieval tasks.\n","I. schizophrenia AND drug\n","II. for AND NOT(drug OR approach)"],"metadata":{"id":"m3pS4wcK_gJF"}},{"cell_type":"code","source":["#schizophrenia AND drug\n","\n","def And_op(list1,list2):\n","  if ((list1) and (list2)):\n","    return set(list1).intersection(list2)\n","  else:\n","      return()\n","\n","def OR_op(list1,list2):\n","  return set(list1).union(list2)\n","\n","def Not_op(a):\n","  directory = os\n","\n","li=InvertedIndex()\n","for key in li:\n","  if key == 'schizophrenia': #dictionary terms\n","    list1=li[key]\n","    print('list1',list1)\n","  if key == 'drug':\n","    list2=li[key]\n","    print('list2',list2)\n","print('schizophrenia AND drug ', And_op(list1,list2))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZPj8XLV-kTt","executionInfo":{"status":"ok","timestamp":1664116521831,"user_tz":-330,"elapsed":14,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"4e8a73f6-8f91-49b3-9352-a08d375d6aaa"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['breakthrough', 'drug', 'for', 'schizophrenia']\n","['new', 'schizophrenia', 'drug']\n","['new', 'approach', 'for', 'treatment', 'of', 'schizophrenia']\n","['new', 'hopes', 'for', 'schizophrenia', 'patients']\n","list2 ['Doc1.txt', 'Doc2.txt']\n","list1 ['Doc1.txt', 'Doc2.txt', 'Doc3.txt', 'Doc4.txt']\n","schizophrenia AND drug  {'Doc2.txt', 'Doc1.txt'}\n"]}]},{"cell_type":"markdown","source":["II. for AND NOT(drug OR approach)"],"metadata":{"id":"3dq8sGh1IHCB"}},{"cell_type":"code","source":["li=InvertedIndex()\n","for key in li:\n","  if key == 'for': #dictionary terms\n","    list1=li[key]\n","    print('list1',list1)\n","  if key == 'drug':\n","    list2=li[key]\n","    print('list2',list2)\n","  if key == 'approach':\n","    list3=li[key]\n","    print('list2',list2)\n","\n","#or operation \n","list4 = OR_op(list2,list3)\n","print(list4)\n","\n","#NOT operator\n","\n","list5 = Not_op(list4)\n","print(list5)\n","\n","#AND , NOT operator\n","\n","list6=  And_op(list1,list5)\n","print(list6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEin-XqVIHRd","executionInfo":{"status":"ok","timestamp":1664117864461,"user_tz":-330,"elapsed":723,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"6f7ebebf-667f-4547-a12d-c0670be2dcec"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['breakthrough', 'drug', 'for', 'schizophrenia']\n","['new', 'schizophrenia', 'drug']\n","['new', 'approach', 'for', 'treatment', 'of', 'schizophrenia']\n","['new', 'hopes', 'for', 'schizophrenia', 'patients']\n","list2 ['Doc1.txt', 'Doc2.txt']\n","list1 ['Doc1.txt', 'Doc3.txt', 'Doc4.txt']\n","list2 ['Doc1.txt', 'Doc2.txt']\n","{'Doc2.txt', 'Doc1.txt', 'Doc3.txt'}\n","None\n","()\n"]}]},{"cell_type":"code","source":["import nltk\n","\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTjts61_JQ08","executionInfo":{"status":"ok","timestamp":1664126245508,"user_tz":-330,"elapsed":1030,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"1dac9758-64ed-45b8-c4e3-59bb6a32ecac"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Positional Indexing "],"metadata":{"id":"e20e1xB7MOIG"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","\n","stop_words_list = stopwords.words('english')\n","print(stop_words_list)\n","print(len(stop_words_list))\n","\n","quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n"," \n","tokenized_word = word_tokenize(quote)\n","\n","print(tokenized_word)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUPIRI_PMQx1","executionInfo":{"status":"ok","timestamp":1664125958809,"user_tz":-330,"elapsed":443,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"04a32614-7b43-474f-ef4d-b3d72b0cba5d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","179\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize \n","\n","quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n"," \n","tokenized_word = word_tokenize(quote)\n","\n","print(tokenized_word)\n","\n","#remove the stop words \n","\n","#1 \n","words = [x for x in tokenized_word if not x.lower() in stop_words_list]\n","print(words)\n","\n","#2 method \n","new_list=[]\n","for word in tokenized_word:\n","  if word not in stop_words_list:\n","    new_list.append(word)\n","\n","print(new_list)\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrAficOapsCo","executionInfo":{"status":"ok","timestamp":1664126863024,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"aedf9ece-d7c0-4a85-9734-b4d790427aa5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n","['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success', '.']\n","['Pythoners', 'intelligent', 'work', 'pythonly', 'pythoning', 'way', 'success', '.']\n"]}]},{"cell_type":"markdown","source":[" Include ‘intelligent, ‘work’ as stopwords and print the new word list after removing stopwords"],"metadata":{"id":"Tbd50UxMsyk_"}},{"cell_type":"code","source":["#Add new stop words \n","\n","stop_words_list = stopwords.words('english')\n","custom_stop_word = ['intelligent','work']\n","final_stop_words = stop_words_list + custom_stop_word\n","print(final_stop_words)\n","print(len(final_stop_words))\n","\n","\n","quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n"," \n","tokenized_word = word_tokenize(quote)\n","\n","new_list=[]\n","for word in tokenized_word:\n","  if word not in final_stop_words:\n","    new_list.append(word)\n","print(new_list)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J4fMX26zs0fH","executionInfo":{"status":"ok","timestamp":1664127334606,"user_tz":-330,"elapsed":384,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"1e807cdd-9c2b-4665-f7d8-e7a016b1d8f6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'intelligent', 'work']\n","181\n","['Pythoners', 'pythonly', 'pythoning', 'way', 'success', '.']\n"]}]},{"cell_type":"markdown","source":["b) Use stemming text processing for the given sentence"],"metadata":{"id":"gXrJKqCVugpQ"}},{"cell_type":"code","source":["#STEMMING \n","\n","#use after tokaniztaion \n","\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","quote = \"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n","\n","ps=PorterStemmer()\n","stemmed_list=[]\n","tokenized_sentence = word_tokenize(quote)\n","\n","for token in tokenized_sentence:\n","  stemmed_list.append(ps.stem(token))\n","\n","print(stemmed_list)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfkO2wy4uhrL","executionInfo":{"status":"ok","timestamp":1664127793228,"user_tz":-330,"elapsed":395,"user":{"displayName":"Sachintha Kumarasinghe","userId":"17393694132893308025"}},"outputId":"676a5ac1-2fbe-4249-d08a-56b309fd6e5c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['python', 'are', 'veri', 'intellig', 'and', 'work', 'veri', 'pythonli', 'and', 'now', 'they', 'are', 'python', 'their', 'way', 'to', 'success', '.']\n"]}]}]}